---
title: "Load and Analyse Truro Network"
author: "Valentin Lucet & Imogen Hobbs"
date: "28/12/2021"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries 

```{r}
library(readxl)
library(Rgraphviz)
library(bnlearn)
```

# Read in data 

```{r}
# The path to data is relative to where the mrd file is located
excel_path <- "data/Markov_Blankets.xlsx"
# excel_path <- "/Users/imogenhobbs/Library/Mobile Documents/com~apple~CloudDocs/Theme 1 ResNet/MSc_Renewable_Resources/R/data/2Markov_Blankets.xlsx"

# Get all the sheet names
sheet_vector <- excel_sheets(excel_path)

# Apply the read_excel function too all the sheets
dat_list <- mapply(FUN = read_excel, path =excel_path, 
                   sheet = sheet_vector)
names(dat_list) <- sheet_vector

# Split the names of the sheets to get info on the variables
sheet_vector_clean_split <- 
  sapply(sheet_vector, function(x) { 
    if(grepl("and", x)) strsplit(x, "_and_", fixed = T) else x})

str(dat_list)
print(sheet_vector_clean_split)
```

# Wrangling data into CPTs

We want to transform the data into probability tables for bnlearn

- tables 1-: simple conversion to matrix with some cleaning steps
- tables 7-8: is 3 and 4-way so more wrangling is needed

```{r}
source("scripts/0_functions.R")
```

We apply the function `clean_tables` to all the sheets.

```{r}
cpt_list <- mapply(FUN = clean_tables, 
                   tbl = dat_list, 
                   element_names = sheet_vector_clean_split)
names(cpt_list) <- sapply(sheet_vector_clean_split, `[`, 1)
print(cpt_list)
```

# Creating the network

We create the network with a string of symbols (see
Scutari).

```{r}
network <- (model2network("[HD][MAR][IJ|HD][EW][TID|MAR:EW][FC][DMK][DMNT|FC:DMK][FL|IJ:TID:DMNT]"))
graphviz.plot(network)
```


# Fitting

We can now fit the network: 

```{r}
bn <- bnlearn::custom.fit(network, cpt_list)
print(bn)
```

# Belief Propagation

In order to use this network to help make decisions about future events, we need to use Belief Propagation. 
Belief Propagation can be done with Exact or Approximate Algorithms. Because this network is small (among other reasons), we are going 
to use an Exact Algorithm with Junction Trees. Junction Trees are are popular method of exact inference and exist in the gRain package. 

Junction Trees require that the network be **moralised**, that is, an undirected graph, where each node is now connected to its Markov Blanket. 

Let's moralise our BN:

```{r}
graphviz.plot(moral(network))
```

From this moral graph, we can isolate 4 Cliques:
C1 = {DMK, FC, DMNT}
C2 = {DMNT,TID, IJ}
C3 = {TID, IJ, FL}
C4 = {DMNT, IJ, FL}

And 3 Separators:
S12 = {DMNT}
S14= {DMNT}
S23 = {TID, IJ}

The following section has quotes from Oxford Bayesian Analysis by Scutari (2017).

Let's say we want to see what happens to the distribution of Tides (TID) and Ice Jams (IJ) given the evidence that DMNT is "Low."

"First, we convert the BN from *bnlearn* to its equivalent in *gRain* with as.grain() and we construct the junction tree with compile()"

```{r}
library(gRain)
junction <- compile(as.grain(bn))
```

"Then, we set the evidence to on the node," in this case, fixing it to Low, "with probability 1 with `setEvidence()`"

```{r}
jDMNT <- setEvidence(junction, nodes = "DMNT", states = "High")
```

"And after that, we can perform our conditional probability query with querygrain(), which also takes care of the belief propagation."

```{r}
TIDxIJ.cpt <- querygrain(jDMNT, nodes = c("TID", "IJ"), type = "joint")
print(TIDxIJ.cpt)
```

Let's try it with Flood-risk (FL), with the evidence being set on different variables:

1. Decision maker knowledge:

```{r}
jDMK <- setEvidence(junction, nodes = "DMK", states = "High_DMK")
FLxDMKcpt <- querygrain(jDMK, nodes = c("FL"), type = "joint")
print(round(FLxDMKcpt, 2))

jDMK2 <- setEvidence(junction, nodes = "DMK", states = "Low_DMK")
FLxDMKcpt2 <- querygrain(jDMK2, nodes = c("FL"), type = "joint")
print(round(FLxDMKcpt2, 2))
```

2. Human development:

```{r}
jHD <- setEvidence(junction, nodes = "HD", states = "Expansion")
FLxHDcpt <- querygrain(jHD, nodes = c("FL"), type = "joint")
print(round(FLxHDcpt, 2))

jHD2 <- setEvidence(junction, nodes = "HD", states = "Return_to_Marshland")
FLxHDcpt2 <- querygrain(jHD2, nodes = c("FL"), type = "joint")
print(round(FLxHDcpt2, 2))
```

3. Dyke Maintenance

```{r}
jDMNT <- setEvidence(junction, nodes = "DMNT", states = "Maintained")
FLxDMNTcpt <- querygrain(jDMNT, nodes = c("FL"), type = "joint")
print(round(FLxDMNTcpt, 2))

jDMNT2 <- setEvidence(junction, nodes = "DMNT", states = "Removed")
FLxDMNTcpt2 <- querygrain(jDMNT2, nodes = c("FL"), type = "joint")
print(round(FLxDMNTcpt2, 2))
```

4. Financial Constraints

```{r}
jFC <- setEvidence(junction, nodes = "FC", states = "Above_Average_FC")
FLxFCcpt <- querygrain(jFC, nodes = c("FL"), type = "joint")
print(round(FLxFCcpt, 2))

jFC2 <-setEvidence(junction, nodes = "FC", states = "Below_Average_FC")
FLxFCcpt2 <- querygrain(jFC2, nodes = c("FL"), type = "joint")
print(round(FLxFCcpt2, 2))
```

5. Tide frequency

```{r}
jTF <- setEvidence(junction, nodes = "TID", states = "Above_Average_TID")
FLxTFcpt <- querygrain(jTF, nodes = c("FL"), type = "joint")
print(round(FLxTFcpt, 2))

jTF2 <-setEvidence(junction, nodes = "TID", states = "Below_Average_TID")
FLxTFcpt2 <- querygrain(jTF2, nodes = c("FL"), type = "joint")
print(round(FLxTFcpt2, 2))
```

6. Saltmarsh status

```{r}
jMAR <- setEvidence(junction, nodes = "MAR", states = "Present_MAR")
FLxMARcpt <- querygrain(jMAR, nodes = c("FL"), type = "joint")
print(round(FLxMARcpt, 2))

jMAR2 <-setEvidence(junction, nodes = "MAR", states = "Absent_MAR")
FLxMARcpt2 <- querygrain(jMAR2, nodes = c("FL"), type = "joint")
print(round(FLxMARcpt2, 2))
```

7. Extreme weather frequency

```{r}
jEW <- setEvidence(junction, nodes = "EW", states = "Above_Average_EW")
FLxEWcpt <- querygrain(jEW, nodes = c("FL"), type = "joint")
print(round(FLxEWcpt, 2))

jEW2 <-setEvidence(junction, nodes = "EW", states = "Below_Average_EW")
FLxEWcpt2 <- querygrain(jEW2, nodes = c("FL"), type = "joint")
print(round(FLxEWcpt2, 2))
```

8. Ice Jam frequency

```{r}
jIJ <- setEvidence(junction, nodes = "IJ", states = "Above_Average_IJ")
FLxIJcpt <- querygrain(jIJ, nodes = c("FL"), type = "joint")
print(round(FLxIJcpt, 2))

jIJ2 <- setEvidence(junction, nodes = "IJ", states = "Below_Average_IJ")
FLxIJcpt2 <- querygrain(jIJ2, nodes = c("FL"), type = "joint")
print(round(FLxIJcpt2, 2))
```

# Scenarios by permutations

```{r}
perms <- gtools::permutations(2, 8, repeats.allowed = T)
perms_list <- lapply(apply(perms, MARGIN = 1, list), unlist)

graph_list <- lapply(perms_list, FUN = generate_combination)
FL <- lapply(graph_list, function(x) c(querygrain(x, nodes = c("FL"))[[1]])) 
scores <- unlist(lapply(FL, function(x) x[2]))
max_ids <- which(scores == max(scores))
percent <- (length(max_ids)/length(perms_list))*100

bests <- graph_list[max_ids]

# > length(bests)
# [1] 32
```

We find that 32 scenarios maximize below average flood risk. Lets look at their evidence.

```{r}
evidences <- lapply(bests, function(x) getEvidence(x))
evidences_dfs <- lapply(evidences, function(x){
  df <- as.data.frame(x)[, c("nodes", "hard_state")]
})
all_evidence <- dplyr::bind_rows(evidences_dfs)

all_evidence_ar <- as.data.frame(table(all_evidence)) |> 
    dplyr::arrange(desc(Freq), nodes, decreasing = F)

filtered <- all_evidence_ar |> 
  dplyr::mutate(imp = Freq/length(bests)*100) |> 
  dplyr::filter(Freq !=0)

print(filtered)
```
